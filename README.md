# VLM-Lite-Demo
````markdown
# ğŸ§  VLM-Lite: A Minimal Visionâ€“Language Model Playground

> A lightweight, educational, and fully runnable Visionâ€“Language Model (VLM) framework â€”  
> featuring a **CLIP-style contrastive learner**, integrated **BLIP image captioning**,  
> plus visualization and demo tools: **retrieval**, **GradCAM**, and a captioned-video demo.  
> Designed for research, teaching, and fast prototyping without heavy data or compute needs.

---

## ğŸŒ Overview

**VLM-Lite** is a compact, modular playground for learning and prototyping vision-language ideas.  
It demonstrates core concepts in modern VLMs (CLIP, BLIP, LISA) with emphasis on:

- clarity and modularity (clean `src/` structure),
- low resource requirements (uses TorchVision CIFAR-10 by default),
- immediate visual demos (captioned video, retrieval grids, GradCAM heatmaps).

Key built-in capabilities:
- ğŸ”¹ CLIP-style contrastive training on CIFAR-10 (`src/train.py`).
- ğŸ”¹ BLIP integration for image captioning and a captioned-video demo (`src/integrations/blip_caption.py`, `src/demo/create_caption_video.py`).
- ğŸ”¹ **CLIP Retrieval**: textâ†’image and imageâ†’text retrieval demo (`src/demo/clip_retrieval.py`).
- ğŸ”¹ **CLIP GradCAM**: attention/importance heatmaps for CLIP-like models (`src/demo/clip_gradcam.py`).
- ğŸ”¹ Optional: BLIP VQA / Gradio UI integration for interactive demos (examples and hooks included).

This repo is ideal for:
- students & researchers experimenting with VLM concepts,
- devs wanting a reproducible, educational PyTorch example,
- presenters preparing interactive demos for talks or tutorials.

---

## ğŸ§© Features (summary)

| Module | Description | Key tech |
|---|---:|---|
| `src/models/clip_like.py` | Minimal CLIP-like encoder (image + simple text embedding) | ResNet (timm), projection heads |
| `src/data/cifar_text.py` | CIFAR-10 wrapper that produces imageâ€“text pairs (class names) | torchvision |
| `src/losses/contrastive.py` | InfoNCE-style symmetric contrastive loss | PyTorch |
| `src/integrations/blip_caption.py` | BLIP wrapper (Hugging Face) with local/offline fallback | transformers |
| `src/demo/create_caption_video.py` | Generate an mp4 with BLIP captions over frames | PIL, OpenCV |
| `src/demo/clip_retrieval.py` | Textâ†’Image / Imageâ†’Text retrieval demo, saves retrieval grid | NumPy, PIL |
| `src/demo/clip_gradcam.py` | Grad-CAM style heatmaps for CLIP-like models | PyTorch, OpenCV |
| `src/train.py` | End-to-end CLIP-like training on CIFAR-10 | AdamW, timm |

---

## âš™ï¸ Installation

```bash
# Clone
git clone https://github.com/yourusername/VLM-Lite.git
cd VLM-Lite

# (Optional) virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
````

### Core Dependencies

* `torch`, `torchvision`, `timm`
* `transformers`, `huggingface_hub` (for BLIP), `Pillow`, `opencv-python`
* `numpy`, `tqdm`, `scikit-learn`, `matplotlib` (for visualization)

> Optional: `open_clip_torch` or `sentence-transformers` if you want more powerful text encoders or pretrained CLIP variants (see **Extending**).

---

## ğŸš€ Quick Start

### 1) Train a CLIP-like model on CIFAR-10

```bash
# recommended: run from repository root
python -m src.train
```

Short run (2 epochs) example output:

```
Epoch 0: loss=3.33, zero-shot acc=0.8806
Epoch 1: loss=2.90, zero-shot acc=0.9115
```

Trained checkpoint is saved to `checkpoints/` (e.g. `checkpoints/clip_like_small.pth`).

---

### 2) Generate BLIP caption video (demo)

* If you have internet/ HF access (auto-download):

```bash
python -m src.demo.create_caption_video
```

* If offline but you previously downloaded BLIP to `./models/blip-base`:

```bash
python -m src.demo.create_caption_video --local_model ./models/blip-base
```

* Offline fallback (use CIFAR labels as captions; no HF download):

```bash
python -m src.demo.create_caption_video --offline
```

Output:

```
Saved video to output/blip_demo.mp4
```

Open `output/blip_demo.mp4` â€” each frame shows a CIFAR image with a caption.

---

## ğŸ” Retrieval Demo (text â†’ image) â€” Usage

Text query example (prefer CIFAR class names for best result):

```bash
python -m src.demo.clip_retrieval --ckpt checkpoints/clip_like_small.pth --query "cat" --topk 5
```

Imageâ†’text example:

```bash
python -m src.demo.clip_retrieval --ckpt checkpoints/clip_like_small.pth --image_path samples/my_image.jpg --topk 5
```

Outputs:

* Grid image saved to `output/retrieval/retrieval_<query>.jpg`.
* Console prints top-k indices, labels and similarity scores.

Notes:

* Retrieval uses model's text encoder for class labels. For arbitrary free-text queries, swap in a more powerful text encoder (see **Extending**).

---

## ğŸ”¥ GradCAM Demo (visualize CLIP attention)

Generate Grad-CAM overlay for an input image:

```bash
python -m src.demo.clip_gradcam --ckpt checkpoints/clip_like_small.pth --image_path samples/cat.png --text "cat"
```

* If `--text` omitted, the script will automatically use model's predicted top-1 class as the GradCAM target.
* Output saved to `output/gradcam/<image>_gradcam.jpg` (or custom `--out`).

Usage patterns:

* Inspect where the model looks for evidence of a class.
* Compare GradCAMs for different target texts.

---

## ğŸ§  Analysis & Probing

* TSNE / embedding visualization: `src/analysis/visualize_embeddings.py`
  Example:

  ```bash
  python -m src.analysis.visualize_embeddings --ckpt checkpoints/clip_like_small.pth --out output/embeddings_tsne.png
  ```

* Linear probe evaluation (freeze encoder, train small classifier):

  ```bash
  python -m src.linear_probe --ckpt checkpoints/clip_like_small.pth --epochs 5
  ```

---

## âš–ï¸ BLIP / Offline / Proxy Notes

* BLIP (Hugging Face) will download weights on first run. If your machine cannot reach Hugging Face:

  * Download model on another machine and copy `./models/blip-base` to this repo root. Then run with `--local_model ./models/blip-base`.
  * Or use `--offline` to use CIFAR class labels as captions for demo purposes.

* If your environment needs a proxy:

```bash
export HTTP_PROXY="http://127.0.0.1:7890"
export HTTPS_PROXY="http://127.0.0.1:7890"
python -m src.demo.create_caption_video
```

---

## ğŸ§© Project Structure

```
VLM-Lite/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                 # dataset wrappers (CIFAR text)
â”‚   â”œâ”€â”€ models/               # clip_like.py
â”‚   â”œâ”€â”€ losses/               # contrastive loss
â”‚   â”œâ”€â”€ integrations/         # blip wrapper with offline fallback
â”‚   â”œâ”€â”€ demo/                 # create_caption_video.py, clip_retrieval.py, clip_gradcam.py
â”‚   â”œâ”€â”€ analysis/             # t-SNE, probes
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ output/                   # auto-generated visuals & videos
â”œâ”€â”€ models/                   # optional: local HF models (e.g. blip-base/)
â”œâ”€â”€ checkpoints/              # saved model checkpoints
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Extending the Project

You can extend VLM-Lite easily:

* **Better text encoder / free-text queries**: integrate `open_clip_torch` or `sentence-transformers` (swap `SimpleTextEncoder`).
* **Use pretrained CLIP**: replace local `clip_like` with an official pre-trained CLIP (OpenAI / open_clip) to improve retrieval across arbitrary sentences.
* **BLIP-2 / VQA / Instruction-following**: add `Salesforce/blip2-flan-t5` or InstructBLIP for visual reasoning and VQA; integrate a small LLM for multi-turn dialogue.
* **Interactive UI**: build a Gradio app `src/app/vlm_gradio.py` that shows caption, GradCAM, and retrieval outputs from an uploaded image.
* **LISA / SegPoint**: add reasoning-driven segmentation (LLMâ†’segmentation) and 3D point-cloud modules (requires separate datasets & compute).

---

## ğŸ“¦ Output Examples

| Task               |                    Output | Note                             |
| ------------------ | ------------------------: | -------------------------------- |
| CLIP training      | console logs & checkpoint | small quick-run by default       |
| BLIP caption video |    `output/blip_demo.mp4` | 16 CIFAR frames + captions       |
| Retrieval          |  `output/retrieval/*.jpg` | top-k grid per query             |
| GradCAM            |    `output/gradcam/*.jpg` | heatmap overlay for given target |

---

## ğŸ§‘â€ğŸ’» Credits & References

This project draws inspiration from:

* **CLIP (OpenAI)** â€” [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
* **open_clip** â€” [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)
* **BLIP (Salesforce)** â€” [https://github.com/salesforce/BLIP](https://github.com/salesforce/BLIP)
* **LISA (reasoning-seg)** â€” [https://github.com/dvlab-research/LISA](https://github.com/dvlab-research/LISA)
* **SegPoint (3D)** â€” [https://github.com/SegPoint](https://github.com/SegPoint)
* **timm** â€” [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)

---

## ğŸ“œ License

MIT License. Note: third-party models (BLIP, CLIP variants) are subject to their original licenses.

---

## â¤ï¸ Acknowledgements

Created by **Yu Zhang** as a compact educational and prototyping toolkit for multimodal research and demonstrations.
If this repo helps you, please â­ the project and share your demos!

```

ä¸­æ–‡ç‰ˆï¼š

````markdown
# ğŸ§  VLM-Liteï¼šè½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹å®éªŒå¹³å°  

> ä¸€ä¸ªé¢å‘å­¦ä¹ ã€ç ”ç©¶ä¸æ¼”ç¤ºçš„è½»é‡çº§ Visionâ€“Language Model (VLM) æ¡†æ¶ã€‚  
> å®ƒé›†æˆäº† **CLIP å¼å›¾æ–‡å¯¹æ¯”å­¦ä¹ **ã€**BLIP å›¾åƒæè¿°ç”Ÿæˆ**ã€  
> ä»¥åŠ **å›¾æ–‡æ£€ç´¢** å’Œ **GradCAM å¯è§†åŒ–** ç­‰å¤šæ¨¡æ€åŠŸèƒ½ã€‚  
> æ— éœ€å¤§è§„æ¨¡æ•°æ®æˆ–è®¡ç®—èµ„æºï¼Œå³å¯ä½“éªŒç°ä»£ VLM çš„æ ¸å¿ƒåŸç†ä¸åº”ç”¨ã€‚

---

## ğŸŒ é¡¹ç›®æ¦‚è¿°  

**VLM-Lite** æ˜¯ä¸€ä¸ªå°å·§ä½†å®Œæ•´çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œ  
ç”¨æœ€ç®€ä»£ç å¤ç°äº†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ **CLIP**ã€**BLIP**ã€**LISA**ï¼‰çš„æ ¸å¿ƒæ€æƒ³ï¼Œ  
å¼ºè°ƒ **æ¨¡å—åŒ–ã€å¯è¯»æ€§å¼ºã€èµ„æºå ç”¨ä½**ã€‚  

ä¸»è¦ç‰¹æ€§ï¼š
- ğŸ”¹ CLIP å¼å›¾æ–‡å¯¹é½æ¨¡å‹ï¼Œæ”¯æŒåœ¨ CIFAR-10 ä¸Šå¿«é€Ÿè®­ç»ƒã€‚  
- ğŸ”¹ é›†æˆ **BLIP** å›¾åƒå­—å¹•ç”Ÿæˆæ¨¡å—ï¼Œå¯ç”Ÿæˆå¸¦å­—å¹•è§†é¢‘ã€‚  
- ğŸ”¹ **CLIP å›¾æ–‡æ£€ç´¢ Demo**ï¼šå®ç° textâ†’image å’Œ imageâ†’text æ£€ç´¢ã€‚  
- ğŸ”¹ **CLIP GradCAM å¯è§†åŒ–**ï¼šå±•ç¤ºæ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸã€‚  
- ğŸ”¹ æ‰€æœ‰ç¤ºä¾‹å‡åŸºäº PyTorch ä¸ TorchVision å†…ç½®æ•°æ®é›†ï¼Œæ— éœ€é¢å¤–é¢„å¤„ç†ã€‚  

éå¸¸é€‚åˆï¼š
- ğŸ“ å­¦ä¹ å¤šæ¨¡æ€å¯¹é½ä¸è¡¨å¾å­¦ä¹ åŸç†çš„å­¦ç”Ÿ / ç ”ç©¶è€…ï¼›  
- âš™ï¸ æƒ³å¿«é€Ÿæ„å»ºå¯è§†åŒ– Demo çš„å¼€å‘è€…ï¼›  
- ğŸ’¡ ç”¨äºè¯¾å ‚æ•™å­¦ã€ç§‘ç ”æ¼”ç¤ºä¸è®ºæ–‡è¡¥å……å®éªŒã€‚  

---

## ğŸ§© åŠŸèƒ½æ¦‚è§ˆ  

| æ¨¡å— | åŠŸèƒ½è¯´æ˜ | ä¸»è¦æŠ€æœ¯ |
|------|-----------|-----------|
| `src/models/clip_like.py` | ç®€åŒ–ç‰ˆ CLIP æ¨¡å‹ï¼ˆå›¾åƒç¼–ç  + æ–‡æœ¬åµŒå…¥ï¼‰ | ResNet (timm), æŠ•å½±å±‚ |
| `src/data/cifar_text.py` | CIFAR-10 å›¾æ–‡é…å¯¹å°è£…ï¼ˆè‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬æ ‡ç­¾ï¼‰ | torchvision |
| `src/losses/contrastive.py` | InfoNCE å¯¹æ¯”æŸå¤± | PyTorch |
| `src/integrations/blip_caption.py` | BLIP å›¾åƒå­—å¹•ç”Ÿæˆï¼ˆå«ç¦»çº¿/é•œåƒæ¨¡å¼ï¼‰ | transformers |
| `src/demo/create_caption_video.py` | ç”Ÿæˆå¸¦å­—å¹•è§†é¢‘ | PIL, OpenCV |
| `src/demo/clip_retrieval.py` | å›¾æ–‡æ£€ç´¢ï¼ˆTextâ†’Image / Imageâ†’Textï¼‰ | NumPy, PIL |
| `src/demo/clip_gradcam.py` | GradCAM çƒ­åŠ›å›¾å¯è§†åŒ– | PyTorch, OpenCV |
| `src/train.py` | CLIP å¼å¯¹æ¯”è®­ç»ƒ | AdamW, timm |

---

## âš™ï¸ å®‰è£…ä¸ç¯å¢ƒé…ç½®  

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/VLM-Lite.git
cd VLM-Lite

# ï¼ˆå¯é€‰ï¼‰åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
source .venv/bin/activate

# å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt
````

### ä¸»è¦ä¾èµ–

* `torch`, `torchvision`, `timm`
* `transformers`, `huggingface_hub`, `Pillow`, `opencv-python`
* `numpy`, `tqdm`, `matplotlib`, `scikit-learn`

> å¯é€‰ï¼šè‹¥è¦ä½¿ç”¨æ›´å¼ºçš„é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼Œå¯å®‰è£… `open_clip_torch` æˆ– `sentence-transformers`ã€‚

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

### 1ï¸âƒ£ åœ¨ CIFAR-10 ä¸Šè®­ç»ƒä¸€ä¸ª CLIP å¼æ¨¡å‹

```bash
python -m src.train
```

ç¤ºä¾‹è¾“å‡ºï¼š

```
Epoch 0: loss=3.33, zero-shot acc=0.8806
Epoch 1: loss=2.90, zero-shot acc=0.9115
```

è®­ç»ƒåçš„æƒé‡å°†ä¿å­˜åœ¨ `checkpoints/clip_like_small.pth`ã€‚

---

### 2ï¸âƒ£ è¿è¡Œ BLIP å›¾åƒå­—å¹•è§†é¢‘ Demo

* è‹¥ç½‘ç»œå¯è®¿é—® Hugging Faceï¼š

```bash
python -m src.demo.create_caption_video
```

* è‹¥å·²åœ¨æœ¬åœ°ä¸‹è½½è¿‡ BLIP æ¨¡å‹ï¼ˆä¾‹å¦‚æ”¾åœ¨ `./models/blip-base`ï¼‰ï¼š

```bash
python -m src.demo.create_caption_video --local_model ./models/blip-base
```

* è‹¥å®Œå…¨ç¦»çº¿ï¼ˆä½¿ç”¨ CIFAR ç±»åˆ«åç§°ä»£æ›¿å­—å¹•ï¼‰ï¼š

```bash
python -m src.demo.create_caption_video --offline
```

è¾“å‡ºï¼š

```
Saved video to output/blip_demo.mp4
```

æ‰“å¼€ `output/blip_demo.mp4`ï¼Œå³å¯çœ‹åˆ°æ¯å¼  CIFAR å›¾ç‰‡é…æœ‰ BLIP è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•ã€‚

---

## ğŸ” CLIP å›¾æ–‡æ£€ç´¢ Demo

**æ–‡æœ¬æ£€ç´¢å›¾ç‰‡ï¼ˆText â†’ Imageï¼‰**

```bash
python -m src.demo.clip_retrieval --ckpt checkpoints/clip_like_small.pth --query "cat" --topk 5
```

**å›¾ç‰‡æ£€ç´¢æ–‡å­—ï¼ˆImage â†’ Textï¼‰**

```bash
python -m src.demo.clip_retrieval --ckpt checkpoints/clip_like_small.pth --image_path samples/my_image.jpg --topk 5
```

è¾“å‡ºï¼š

* æ£€ç´¢ç»“æœå¯è§†åŒ–å›¾ä¿å­˜åœ¨ `output/retrieval/retrieval_<query>.jpg`ï¼›
* ç»ˆç«¯æ‰“å° top-k ç»“æœçš„æ ‡ç­¾ä¸ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

> æç¤ºï¼šè‹¥ä½¿ç”¨è‡ªå®šä¹‰æ–‡æœ¬æŸ¥è¯¢ï¼Œè¯·ç¡®ä¿æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨è¶³å¤Ÿæ³›åŒ–ï¼ˆæˆ–æ›¿æ¢ä¸º BERT/CLIP ç¼–ç å™¨ï¼‰ã€‚

---

## ğŸ”¥ CLIP GradCAM å¯è§†åŒ–

ç”Ÿæˆçƒ­åŠ›å›¾å åŠ æ•ˆæœï¼š

```bash
python -m src.demo.clip_gradcam --ckpt checkpoints/clip_like_small.pth --image_path samples/cat.png --text "cat"
```

* è‹¥çœç•¥ `--text`ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨é€‰æ‹©æ¨¡å‹é¢„æµ‹çš„ top-1 ç±»åˆ«ï¼›
* è¾“å‡ºå›¾ç‰‡ä¿å­˜åœ¨ `output/gradcam/<image>_gradcam.jpg`ã€‚

ç”¨é€”ï¼š

* å¯è§†åŒ–æ¨¡å‹å…³æ³¨åŒºåŸŸï¼›
* å¯¹æ¯”ä¸åŒæ–‡æœ¬ç›®æ ‡ä¸‹çš„æ³¨æ„åŠ›å·®å¼‚ã€‚

---

## ğŸ§  åˆ†æä¸æ‰©å±•å®éªŒ

* **ç‰¹å¾å¯è§†åŒ– (t-SNE)**ï¼š

```bash
python -m src.analysis.visualize_embeddings --ckpt checkpoints/clip_like_small.pth --out output/embeddings_tsne.png
```

* **çº¿æ€§æ¢é’ˆ (Linear Probe)**ï¼š

```bash
python -m src.linear_probe --ckpt checkpoints/clip_like_small.pth --epochs 5
```

---

## âš–ï¸ BLIP / ç¦»çº¿ / ä»£ç†è¯´æ˜

* è‹¥æ— æ³•è®¿é—® Hugging Faceï¼Œå¯æ‰‹åŠ¨ä¸‹è½½ `Salesforce/blip-image-captioning-base` å¹¶æ”¾å…¥ `./models/blip-base`ï¼š

  ```bash
  python -m src.demo.create_caption_video --local_model ./models/blip-base
  ```
* å®Œå…¨ç¦»çº¿æ¨¡å¼ï¼š

  ```bash
  python -m src.demo.create_caption_video --offline
  ```
* è‹¥éœ€è¦ä»£ç†ï¼š

  ```bash
  export HTTP_PROXY="http://127.0.0.1:7890"
  export HTTPS_PROXY="http://127.0.0.1:7890"
  ```

---

## ğŸ§° é¡¹ç›®ç»“æ„

```
VLM-Lite/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                 # CIFAR-10 æ•°æ®é›†å°è£…
â”‚   â”œâ”€â”€ models/               # clip_like æ¨¡å‹
â”‚   â”œâ”€â”€ losses/               # å¯¹æ¯”æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ integrations/         # BLIP æ¥å£
â”‚   â”œâ”€â”€ demo/                 # create_caption_video / clip_retrieval / clip_gradcam
â”‚   â”œâ”€â”€ analysis/             # å¯è§†åŒ–ä¸æ¢é’ˆè„šæœ¬
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ checkpoints/              # è®­ç»ƒæ¨¡å‹ä¿å­˜ç›®å½•
â”œâ”€â”€ models/                   # æœ¬åœ°ä¸‹è½½çš„ BLIP æ¨¡å‹
â”œâ”€â”€ output/                   # è§†é¢‘ã€çƒ­å›¾ã€æ£€ç´¢ç»“æœç­‰è¾“å‡º
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“¦ è¾“å‡ºç¤ºä¾‹

| ä»»åŠ¡      | è¾“å‡º                       | è¯´æ˜                  |
| ------- | ------------------------ | ------------------- |
| CLIP è®­ç»ƒ | æ§åˆ¶å°æ—¥å¿— / checkpoints      | å¿«é€Ÿ 2 è½®è®­ç»ƒå³å¯è¿è¡Œ        |
| BLIP è§†é¢‘ | `output/blip_demo.mp4`   | æ¯å¸§ä¸º CIFAR å›¾åƒ + ç”Ÿæˆå­—å¹• |
| å›¾æ–‡æ£€ç´¢    | `output/retrieval/*.jpg` | æ£€ç´¢ç»“æœå¯è§†åŒ–             |
| GradCAM | `output/gradcam/*.jpg`   | CLIP æ³¨æ„åŠ›çƒ­åŠ›å›¾         |

---

## ğŸ” æ‰©å±•æ–¹å‘

VLM-Lite å¯ä»¥è½»æ¾æ‰©å±•ï¼š

| æ–¹å‘      | ç¤ºä¾‹                          | å‚è€ƒé¡¹ç›®                      |
| ------- | --------------------------- | ------------------------- |
| æ›´å¼ºæ–‡æœ¬ç¼–ç å™¨ | æ›¿æ¢ä¸º BERTã€GPT-2 æˆ– CLIP å®˜æ–¹æ–‡æœ¬å¡” | Hugging Face Transformers |
| æ›´é«˜å±‚ç”Ÿæˆæ¨¡å‹ | é›†æˆ BLIP-2 / CoCaï¼Œæ”¯æŒè§†è§‰æ¨ç†ä¸é—®ç­”  | Salesforce/BLIP-2         |
| äº¤äº’å¼åº”ç”¨   | é€šè¿‡ Gradio åˆ›å»ºåœ¨çº¿å¯è§†åŒ–ç•Œé¢         | Gradio                    |
| æ¨ç†å‹åˆ†å‰²   | æ¥å…¥ LISA æ¡†æ¶ï¼Œå®ç°æ–‡å­—é©±åŠ¨åˆ†å‰²         | dvlab-research/LISA       |
| ä¸‰ç»´æ¨ç†    | æ‰©å±•ä¸º SegPoint ç‚¹äº‘ç†è§£           | SegPoint Project          |

---

## ğŸ§‘â€ğŸ’» è‡´è°¢ä¸å¼•ç”¨

æœ¬é¡¹ç›®å‚è€ƒå¹¶å­¦ä¹ è‡ªä»¥ä¸‹ä¼˜ç§€å¼€æºå·¥ä½œï¼š

* **CLIP (OpenAI)** â€” [OpenAI/CLIP](https://github.com/openai/CLIP)
* **open_clip** â€” [mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)
* **BLIP** â€” [Salesforce/BLIP](https://github.com/salesforce/BLIP)
* **LISA** â€” [dvlab-research/LISA](https://github.com/dvlab-research/LISA)
* **SegPoint** â€” [SegPoint Project](https://github.com/SegPoint)
* **timm** â€” [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)

---

## ğŸ“œ å¼€æºåè®®

æœ¬é¡¹ç›®åŸºäº **MIT License** å¼€æºã€‚
BLIP ç­‰æ¨¡å‹éœ€éµå¾ªå…¶åŸå§‹å¼€æºåè®®ï¼ˆSalesforce / Hugging Faceï¼‰ã€‚

---

## â¤ï¸ ä½œè€…è¯´æ˜

æœ¬é¡¹ç›®ç”± **Yu Zhang** åˆ›å»ºï¼Œ
ä½œä¸º CVPR ç ”ç©¶åŸå‹ä¸æ•™å­¦ç¤ºä¾‹ï¼Œ
æ—¨åœ¨å¸®åŠ©ç§‘ç ”ä¸å¼€å‘äººå‘˜å¿«é€Ÿç†è§£è§†è§‰è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæœºåˆ¶ä¸å¤šæ¨¡æ€åº”ç”¨ã€‚

> å¦‚æœä½ è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·ç‚¹ä¸ª â­ æ”¯æŒä¸€ä¸‹å§ï¼

```

